# Redis ìºì‹± ì „ëµ

## ğŸ“Œ ë¬¸ì„œ ëª©ì 

SKKU-INSIGHTì˜ Redis ìºì‹± ì „ëµì„ ì •ì˜í•˜ê³ , ìºì‹œ í‚¤ ì„¤ê³„, TTL ì •ì±…, ë¬´íš¨í™” ì „ëµ, ë©”ëª¨ë¦¬ ê´€ë¦¬ ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## ğŸ¯ ìºì‹± ëª©ì 

### 1. ì„±ëŠ¥ í–¥ìƒ
- **API ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•**: DB ì¿¼ë¦¬ â†’ Redis ì¡°íšŒ (10ms ì´ë‚´)
- **LLM ë¹„ìš© ì ˆê°**: ë™ì¼ ìš”ì²­ ìºì‹œ ë°˜í™˜ (80% ì ˆê°)

### 2. ë¶€í•˜ ê°ì†Œ
- **DB ë¶€í•˜ ê°ì†Œ**: ë°˜ë³µ ì¿¼ë¦¬ ì œê±°
- **ì™¸ë¶€ API í˜¸ì¶œ ìµœì†Œí™”**: pykrx, Gemini API

### 3. ì‚¬ìš©ì ê²½í—˜
- **ì¦‰ì‹œ ì‘ë‹µ**: ìºì‹œ íˆíŠ¸ ì‹œ ì¦‰ê° ë°˜í™˜
- **ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸**: TTL ê´€ë¦¬ë¡œ ìµœì‹  ë°ì´í„° ë³´ì¥

---

## ğŸ—‚ï¸ Redis ë°ì´í„° êµ¬ì¡°

### ì„ íƒí•œ ë°ì´í„° íƒ€ì…

| ë°ì´í„° | Redis íƒ€ì… | ì´ìœ  |
|--------|-----------|------|
| ê¸‰ë“±ì£¼ ëª©ë¡ | String (JSON) | ì „ì²´ ì¡°íšŒ |
| ê¸°ì—… ë¶„ì„ | String (JSON) | ë³µì¡í•œ ì¤‘ì²© êµ¬ì¡° |
| ì¢…ëª© ê¸°ë³¸ ì •ë³´ | Hash | í•„ë“œë³„ ì¡°íšŒ ê°€ëŠ¥ |
| Rate Limiting | String (Counter) | ê°„ë‹¨í•œ ì¦ê° |
| ì„¸ì…˜ ë°ì´í„° | Hash | ì‚¬ìš©ìë³„ ìƒíƒœ |

---

## ğŸ”‘ ìºì‹œ í‚¤ ì„¤ê³„

### ë„¤ì´ë° ê·œì¹™
```
{service}:{resource}:{identifier}:{sub_identifier}
```

### í‚¤ ëª©ë¡

#### 1. ê¸‰ë“±ì£¼ íŠ¸ë¦¬ê±°
```python
# ì „ì²´ ì„¸ì…˜
"triggers:morning:2025-11-06"
"triggers:afternoon:2025-11-06"

# íŠ¹ì • íŠ¸ë¦¬ê±° íƒ€ì…
"triggers:morning:2025-11-06:volume_surge"
"triggers:afternoon:2025-11-06:closing_strength"

# ì¢…ëª©ë³„ íˆìŠ¤í† ë¦¬
"triggers:history:005930"  # List íƒ€ì…
```

**ë°ì´í„° êµ¬ì¡°**:
```json
{
  "session": "morning",
  "date": "2025-11-06",
  "generated_at": "2025-11-06T09:15:23",
  "triggers": [
    {
      "type": "volume_surge",
      "stocks": [...]
    }
  ]
}
```

**TTL**: 1ì‹œê°„ (3600ì´ˆ)

#### 2. ê¸°ì—… ë¶„ì„
```python
# ê¸°ì—… ë¶„ì„ ê²°ê³¼
"analysis:005930:2025-11-06"

# ê°„ë‹¨ ìš”ì•½ (ë¹ ë¥¸ ì¡°íšŒìš©)
"analysis:summary:005930:2025-11-06"
```

**ë°ì´í„° êµ¬ì¡°**:
```json
{
  "ticker": "005930",
  "date": "2025-11-06",
  "investment_opinion": "BUY",
  "target_price": 85000,
  "current_price": 75000,
  "upside_potential": 13.33,
  "analysis": {...},  // ì „ì²´ ë¶„ì„ ë‚´ìš©
  "metadata": {
    "generated_at": "2025-11-06T10:30:15",
    "model": "gemini-2.5-flash",
    "tokens_used": 1850
  }
}
```

**TTL**: 24ì‹œê°„ (86400ì´ˆ)

#### 3. ì¥ ë¦¬í¬íŠ¸
```python
"report:morning:2025-11-06"
"report:afternoon:2025-11-06"
```

**TTL**: 12ì‹œê°„ (43200ì´ˆ)

#### 4. ì¢…ëª© ê¸°ë³¸ ì •ë³´
```python
# Hash íƒ€ì…
"stock:info:005930"
```

**ë°ì´í„° êµ¬ì¡°** (Hash):
```
HSET stock:info:005930 ticker "005930"
HSET stock:info:005930 name "ì‚¼ì„±ì „ì"
HSET stock:info:005930 market "KOSPI"
HSET stock:info:005930 sector "IT/ë°˜ë„ì²´"
```

**TTL**: 7ì¼ (604800ì´ˆ)

#### 5. ê°€ê²© ë°ì´í„°
```python
# ìµœê·¼ 30ì¼ ê°€ê²© (List íƒ€ì…)
"price:history:005930"
```

**ë°ì´í„° êµ¬ì¡°** (List of JSON):
```json
[
  {"date": "2025-11-06", "open": 73500, "high": 76000, "low": 73000, "close": 75000},
  {"date": "2025-11-05", "open": 72000, "high": 74000, "low": 71500, "close": 73000}
]
```

**TTL**: 1ì‹œê°„ (3600ì´ˆ)

#### 6. Rate Limiting
```python
# IP ê¸°ë°˜ ì œí•œ
"rate_limit:ip:192.168.1.1:2025-11-06:14:30"  # ë¶„ ë‹¨ìœ„
"rate_limit:api:analysis:192.168.1.1:2025-11-06:14"  # ì‹œê°„ ë‹¨ìœ„
```

**ë°ì´í„° êµ¬ì¡°**: ìˆ«ì (Counter)
```
SET rate_limit:ip:192.168.1.1:2025-11-06:14:30 1
INCR rate_limit:ip:192.168.1.1:2025-11-06:14:30
```

**TTL**: 60ì´ˆ (ë¶„ ë‹¨ìœ„) / 3600ì´ˆ (ì‹œê°„ ë‹¨ìœ„)

#### 7. LLM ìš”ì²­ ì¶”ì 
```python
"llm:requests:2025-11-06"  # Hash íƒ€ì…
```

**ë°ì´í„° êµ¬ì¡°**:
```
HSET llm:requests:2025-11-06 count 150
HSET llm:requests:2025-11-06 tokens 280000
HSET llm:requests:2025-11-06 cost 2.8
```

**TTL**: 30ì¼ (2592000ì´ˆ)

---

## â±ï¸ TTL ì •ì±…

### TTL ê²°ì • ê¸°ì¤€

| ë°ì´í„° | TTL | ì´ìœ  |
|--------|-----|------|
| ê¸‰ë“±ì£¼ íŠ¸ë¦¬ê±° | 1ì‹œê°„ | ì¥ ì¤‘ ë³€ë™ì„± |
| ê¸°ì—… ë¶„ì„ | 24ì‹œê°„ | ì¼ì¼ ì—…ë°ì´íŠ¸ ì¶©ë¶„ |
| ì¥ ë¦¬í¬íŠ¸ | 12ì‹œê°„ | í•˜ë£¨ 2íšŒ ìƒì„± |
| ì¢…ëª© ì •ë³´ | 7ì¼ | ê±°ì˜ ë³€ê²½ ì—†ìŒ |
| ê°€ê²© íˆìŠ¤í† ë¦¬ | 1ì‹œê°„ | ì‹¤ì‹œê°„ ë°˜ì˜ |
| Rate Limit | 60ì´ˆ | ë¶„ë‹¹ ì œí•œ |

### TTL êµ¬í˜„

```python
import redis
from datetime import timedelta

class CacheService:
    def __init__(self):
        self.redis = redis.Redis(
            host='localhost',
            port=6379,
            db=0,
            decode_responses=True
        )

    def set_triggers(self, session: str, date: str, data: dict):
        """ê¸‰ë“±ì£¼ ìºì‹± (TTL: 1ì‹œê°„)"""
        key = f"triggers:{session}:{date}"
        self.redis.setex(
            key,
            timedelta(hours=1),
            json.dumps(data, ensure_ascii=False)
        )

    def set_analysis(self, ticker: str, date: str, data: dict):
        """ê¸°ì—… ë¶„ì„ ìºì‹± (TTL: 24ì‹œê°„)"""
        key = f"analysis:{ticker}:{date}"
        self.redis.setex(
            key,
            timedelta(days=1),
            json.dumps(data, ensure_ascii=False)
        )

    def set_report(self, report_type: str, date: str, data: dict):
        """ì¥ ë¦¬í¬íŠ¸ ìºì‹± (TTL: 12ì‹œê°„)"""
        key = f"report:{report_type}:{date}"
        self.redis.setex(
            key,
            timedelta(hours=12),
            json.dumps(data, ensure_ascii=False)
        )
```

### TTL ì—°ì¥ ì „ëµ

```python
def extend_ttl_if_popular(key: str):
    """
    ì¸ê¸° ìˆëŠ” ì¢…ëª©ì€ TTL ì—°ì¥
    (ì¡°íšŒ íšŸìˆ˜ê°€ ì„ê³„ê°’ ì´ìƒì¼ ë•Œ)
    """
    views_key = f"{key}:views"
    views = int(redis.get(views_key) or 0)

    if views > 100:  # 100íšŒ ì´ìƒ ì¡°íšŒ
        # TTLì„ 2ë°°ë¡œ ì—°ì¥
        current_ttl = redis.ttl(key)
        if current_ttl > 0:
            redis.expire(key, current_ttl * 2)
```

---

## ğŸ”„ ìºì‹œ ë¬´íš¨í™” ì „ëµ

### 1. TTL ê¸°ë°˜ ìë™ ë§Œë£Œ
ê°€ì¥ ê°„ë‹¨í•˜ê³  ì•ˆì „í•œ ë°©ë²•

```python
# ìë™ìœ¼ë¡œ ë§Œë£Œë¨
redis.setex("analysis:005930:2025-11-06", 86400, data)
```

### 2. ì´ë²¤íŠ¸ ê¸°ë°˜ ë¬´íš¨í™”

#### ì¤‘ìš” ê³µì‹œ ë°œìƒ ì‹œ
```python
def invalidate_on_disclosure(ticker: str):
    """
    DART API ì›¹í›…ìœ¼ë¡œ ì¤‘ìš” ê³µì‹œ ê°ì§€ ì‹œ
    í•´ë‹¹ ì¢…ëª©ì˜ ë¶„ì„ ìºì‹œ ì‚­ì œ
    """
    pattern = f"analysis:{ticker}:*"
    keys = redis.keys(pattern)
    if keys:
        redis.delete(*keys)
        logger.info(f"Invalidated {len(keys)} cache keys for {ticker}")
```

#### ê¸‰ë“±/ê¸‰ë½ ë°œìƒ ì‹œ
```python
def invalidate_on_price_change(ticker: str, change_rate: float):
    """
    10% ì´ìƒ ê¸‰ë“±/ê¸‰ë½ ì‹œ ìºì‹œ ë¬´íš¨í™”
    """
    if abs(change_rate) >= 10:
        redis.delete(f"analysis:{ticker}:*")
        logger.warning(f"Price shock detected for {ticker}: {change_rate}%")
```

#### ì‹œê°„ ê¸°ë°˜ ë¬´íš¨í™”
```python
from apscheduler.schedulers.background import BackgroundScheduler

def scheduled_invalidation():
    """
    ë§¤ì¼ ìì •: ë§Œë£Œëœ ìºì‹œ ì •ë¦¬
    ë§¤ì£¼ ì¼ìš”ì¼: ì „ì²´ ìºì‹œ ê°±ì‹ 
    """
    scheduler = BackgroundScheduler()

    # ë§¤ì¼ ìì • ì‹¤í–‰
    scheduler.add_job(
        cleanup_expired_cache,
        trigger="cron",
        hour=0,
        minute=0
    )

    # ë§¤ì£¼ ì¼ìš”ì¼ ìì •
    scheduler.add_job(
        refresh_all_cache,
        trigger="cron",
        day_of_week="sun",
        hour=0
    )

    scheduler.start()
```

### 3. ë²„ì „ ê¸°ë°˜ ë¬´íš¨í™”

```python
# ìºì‹œ í‚¤ì— ë²„ì „ í¬í•¨
CACHE_VERSION = "v2"

def get_versioned_key(base_key: str) -> str:
    return f"{base_key}:{CACHE_VERSION}"

# ë²„ì „ ë³€ê²½ ì‹œ ìë™ìœ¼ë¡œ ì´ì „ ìºì‹œ ë¬´íš¨í™”
key = get_versioned_key("analysis:005930:2025-11-06")
```

### 4. Tag ê¸°ë°˜ ë¬´íš¨í™”

```python
def tag_cache(key: str, tags: list):
    """
    ìºì‹œì— íƒœê·¸ ì¶”ê°€ (ì§‘í•© ì‚¬ìš©)
    """
    for tag in tags:
        redis.sadd(f"tag:{tag}", key)

def invalidate_by_tag(tag: str):
    """
    íŠ¹ì • íƒœê·¸ì˜ ëª¨ë“  ìºì‹œ ì‚­ì œ
    """
    keys = redis.smembers(f"tag:{tag}")
    if keys:
        redis.delete(*keys)
        redis.delete(f"tag:{tag}")

# ì‚¬ìš© ì˜ˆì‹œ
tag_cache("analysis:005930:2025-11-06", ["sector:IT", "market:KOSPI"])
invalidate_by_tag("sector:IT")  # IT ì„¹í„° ì „ì²´ ë¬´íš¨í™”
```

---

## ğŸ’¾ ë©”ëª¨ë¦¬ ê´€ë¦¬

### 1. ìµœëŒ€ ë©”ëª¨ë¦¬ ì„¤ì •

```conf
# redis.conf
maxmemory 2gb
maxmemory-policy allkeys-lru  # LRU ì •ì±…
```

### 2. ë©”ëª¨ë¦¬ ì •ì±…

| ì •ì±… | ì„¤ëª… | ì í•©ì„± |
|------|------|--------|
| noeviction | ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì—ëŸ¬ | âŒ ì„œë¹„ìŠ¤ ì¤‘ë‹¨ |
| allkeys-lru | ëª¨ë“  í‚¤ì—ì„œ LRU ì œê±° | âœ… ì¶”ì²œ |
| volatile-lru | TTL ìˆëŠ” í‚¤ì—ì„œ LRU | âš ï¸ TTL ì—†ìœ¼ë©´ ë¬¸ì œ |
| allkeys-random | ëœë¤ ì œê±° | âŒ ë¹„íš¨ìœ¨ì  |

**ì„ íƒ**: `allkeys-lru` (ê°€ì¥ ì˜¤ë˜ ì‚¬ìš©í•˜ì§€ ì•Šì€ í‚¤ ì œê±°)

### 3. ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§

```python
def check_memory_usage():
    """Redis ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬"""
    info = redis.info('memory')

    used_memory_mb = info['used_memory'] / (1024 * 1024)
    max_memory_mb = info['maxmemory'] / (1024 * 1024)
    usage_percent = (used_memory_mb / max_memory_mb) * 100

    logger.info(f"Redis Memory: {used_memory_mb:.2f}MB / {max_memory_mb:.2f}MB ({usage_percent:.1f}%)")

    # 80% ì´ìƒ ì‚¬ìš© ì‹œ ê²½ê³ 
    if usage_percent >= 80:
        logger.warning("Redis memory usage is high!")
        # ìˆ˜ë™ ì •ë¦¬ íŠ¸ë¦¬ê±°
        cleanup_old_cache()

    return {
        "used_mb": used_memory_mb,
        "max_mb": max_memory_mb,
        "usage_percent": usage_percent
    }
```

### 4. ìºì‹œ ì••ì¶•

```python
import gzip
import json

def compress_cache(data: dict) -> bytes:
    """í° ë°ì´í„°ëŠ” ì••ì¶•í•˜ì—¬ ì €ì¥"""
    json_str = json.dumps(data, ensure_ascii=False)
    compressed = gzip.compress(json_str.encode('utf-8'))
    return compressed

def decompress_cache(compressed: bytes) -> dict:
    """ì••ì¶• í•´ì œ"""
    decompressed = gzip.decompress(compressed)
    return json.loads(decompressed.decode('utf-8'))

# ì‚¬ìš© ì˜ˆì‹œ
def set_large_data(key: str, data: dict):
    if len(json.dumps(data)) > 100 * 1024:  # 100KB ì´ìƒ
        compressed = compress_cache(data)
        redis.setex(f"{key}:compressed", 86400, compressed)
    else:
        redis.setex(key, 86400, json.dumps(data))
```

---

## ğŸ“Š ìºì‹œ ì„±ëŠ¥ ì¸¡ì •

### 1. Hit Rate ê³„ì‚°

```python
class CacheMetrics:
    def __init__(self):
        self.hits = 0
        self.misses = 0

    def record_hit(self):
        self.hits += 1

    def record_miss(self):
        self.misses += 1

    def get_hit_rate(self) -> float:
        total = self.hits + self.misses
        if total == 0:
            return 0.0
        return self.hits / total

    def reset(self):
        self.hits = 0
        self.misses = 0

# ì „ì—­ metrics ê°ì²´
cache_metrics = CacheMetrics()

# ì‚¬ìš© ì˜ˆì‹œ
def get_cached_analysis(ticker: str, date: str) -> dict:
    key = f"analysis:{ticker}:{date}"
    cached = redis.get(key)

    if cached:
        cache_metrics.record_hit()
        return json.loads(cached)
    else:
        cache_metrics.record_miss()
        return None
```

### 2. ì‘ë‹µ ì‹œê°„ ì¸¡ì •

```python
import time

def measure_cache_performance():
    """ìºì‹œ vs DB ì„±ëŠ¥ ë¹„êµ"""

    # ìºì‹œ ì¡°íšŒ
    start = time.time()
    cached = redis.get("analysis:005930:2025-11-06")
    cache_time = (time.time() - start) * 1000  # ms

    # DB ì¡°íšŒ
    start = time.time()
    db_result = db.query("SELECT * FROM analysis WHERE ticker='005930'")
    db_time = (time.time() - start) * 1000

    logger.info(f"Cache: {cache_time:.2f}ms, DB: {db_time:.2f}ms")
    logger.info(f"Speedup: {db_time / cache_time:.1f}x")
```

---

## ğŸ” ìºì‹œ ë³´ì•ˆ

### 1. ë¯¼ê° ë°ì´í„° ì•”í˜¸í™”

```python
from cryptography.fernet import Fernet

class SecureCache:
    def __init__(self, encryption_key: bytes):
        self.fernet = Fernet(encryption_key)

    def set_secure(self, key: str, data: dict, ttl: int):
        """ë¯¼ê°í•œ ë°ì´í„° ì•”í˜¸í™” ì €ì¥"""
        json_str = json.dumps(data)
        encrypted = self.fernet.encrypt(json_str.encode())
        redis.setex(f"secure:{key}", ttl, encrypted)

    def get_secure(self, key: str) -> dict:
        """ë³µí˜¸í™” ì¡°íšŒ"""
        encrypted = redis.get(f"secure:{key}")
        if encrypted:
            decrypted = self.fernet.decrypt(encrypted)
            return json.loads(decrypted.decode())
        return None
```

### 2. Redis ì¸ì¦

```conf
# redis.conf
requirepass your_strong_password_here
```

```python
redis = Redis(
    host='localhost',
    port=6379,
    password=os.getenv('REDIS_PASSWORD')
)
```

---

## ğŸš€ ê³ ê¸‰ ìºì‹± ì „ëµ

### 1. Cache Warming (ì‚¬ì „ ìºì‹±)

```python
async def warm_cache_before_market_open():
    """
    ì¥ ì‹œì‘ 30ë¶„ ì „ (08:30)
    ì¸ê¸° ì¢…ëª© ì‚¬ì „ ìºì‹±
    """
    popular_tickers = ["005930", "000660", "035420", "005380", "051910"]

    tasks = []
    for ticker in popular_tickers:
        task = analyze_and_cache(ticker)
        tasks.append(task)

    await asyncio.gather(*tasks)
    logger.info(f"Warmed cache for {len(popular_tickers)} stocks")
```

### 2. Cache Aside Pattern

```python
async def get_analysis_with_cache_aside(ticker: str, date: str) -> dict:
    """
    1. ìºì‹œ ì¡°íšŒ
    2. ì—†ìœ¼ë©´ DB ì¡°íšŒ
    3. DBì—ë„ ì—†ìœ¼ë©´ LLM ìƒì„±
    4. ê²°ê³¼ë¥¼ ìºì‹œ ë° DBì— ì €ì¥
    """
    # 1. ìºì‹œ ì¡°íšŒ
    cache_key = f"analysis:{ticker}:{date}"
    cached = redis.get(cache_key)
    if cached:
        return json.loads(cached)

    # 2. DB ì¡°íšŒ
    db_result = db.query(
        "SELECT * FROM analysis WHERE ticker=? AND date=?",
        (ticker, date)
    )
    if db_result:
        # DB ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        redis.setex(cache_key, 86400, json.dumps(db_result))
        return db_result

    # 3. LLM ìƒì„±
    analysis = await generate_analysis_with_llm(ticker)

    # 4. ìºì‹œ ë° DB ì €ì¥
    redis.setex(cache_key, 86400, json.dumps(analysis))
    db.insert("analysis", analysis)

    return analysis
```

### 3. Write-Through Cache

```python
def save_analysis_with_write_through(ticker: str, date: str, analysis: dict):
    """
    DB ì €ì¥ê³¼ ë™ì‹œì— ìºì‹œ ì—…ë°ì´íŠ¸
    """
    # 1. DB ì €ì¥
    db.insert("analysis", analysis)

    # 2. ìºì‹œ ì—…ë°ì´íŠ¸
    cache_key = f"analysis:{ticker}:{date}"
    redis.setex(cache_key, 86400, json.dumps(analysis))

    logger.info(f"Saved analysis for {ticker} to DB and cache")
```

### 4. Lazy Loading

```python
def get_stock_info(ticker: str) -> dict:
    """
    ìš”ì²­ ì‹œì ì— ìºì‹œ ìƒì„± (Lazy)
    """
    cache_key = f"stock:info:{ticker}"
    cached = redis.get(cache_key)

    if cached:
        return json.loads(cached)

    # ìºì‹œ ë¯¸ìŠ¤ â†’ DB ì¡°íšŒ
    stock_info = db.query("SELECT * FROM stocks WHERE ticker=?", (ticker,))

    if stock_info:
        # ìºì‹œ ì €ì¥ (Lazy)
        redis.setex(cache_key, 604800, json.dumps(stock_info))

    return stock_info
```

---

## ğŸ§ª ìºì‹œ í…ŒìŠ¤íŠ¸

### 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
import pytest

class TestCacheService:
    def test_set_and_get_triggers(self):
        """íŠ¸ë¦¬ê±° ìºì‹± í…ŒìŠ¤íŠ¸"""
        data = {"session": "morning", "triggers": [...]}
        cache.set_triggers("morning", "2025-11-06", data)

        retrieved = cache.get_triggers("morning", "2025-11-06")
        assert retrieved == data

    def test_ttl_expiration(self):
        """TTL ë§Œë£Œ í…ŒìŠ¤íŠ¸"""
        cache.set_triggers("morning", "2025-11-06", {"test": "data"})

        # 1ì‹œê°„ í›„ ì‹œë®¬ë ˆì´ì…˜ (freezegun ì‚¬ìš©)
        with freeze_time("2025-11-06 10:15:00"):
            result = cache.get_triggers("morning", "2025-11-06")
            assert result is None  # ë§Œë£Œë¨

    def test_cache_invalidation(self):
        """ìºì‹œ ë¬´íš¨í™” í…ŒìŠ¤íŠ¸"""
        cache.set_analysis("005930", "2025-11-06", {"test": "data"})
        cache.invalidate_analysis("005930")

        result = cache.get_analysis("005930", "2025-11-06")
        assert result is None
```

### 2. ë¶€í•˜ í…ŒìŠ¤íŠ¸

```python
import asyncio
import time

async def load_test_cache():
    """
    1000ê°œ ë™ì‹œ ìš”ì²­ìœ¼ë¡œ ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    """
    async def single_request():
        start = time.time()
        result = cache.get_analysis("005930", "2025-11-06")
        elapsed = (time.time() - start) * 1000
        return elapsed

    # 1000ê°œ ë™ì‹œ ìš”ì²­
    tasks = [single_request() for _ in range(1000)]
    response_times = await asyncio.gather(*tasks)

    # í†µê³„
    avg_time = sum(response_times) / len(response_times)
    max_time = max(response_times)
    min_time = min(response_times)

    logger.info(f"Avg: {avg_time:.2f}ms, Min: {min_time:.2f}ms, Max: {max_time:.2f}ms")
```

---

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

### Prometheus + Grafana

```python
from prometheus_client import Counter, Histogram, Gauge

# ë©”íŠ¸ë¦­ ì •ì˜
cache_hits = Counter('cache_hits_total', 'Total cache hits')
cache_misses = Counter('cache_misses_total', 'Total cache misses')
cache_latency = Histogram('cache_latency_seconds', 'Cache access latency')
cache_memory = Gauge('cache_memory_bytes', 'Current cache memory usage')

# ì‚¬ìš© ì˜ˆì‹œ
def get_with_metrics(key: str):
    with cache_latency.time():
        result = redis.get(key)

        if result:
            cache_hits.inc()
        else:
            cache_misses.inc()

        return result
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Redis Documentation](https://redis.io/docs/)
- [Caching Strategies](https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html)
- [Redis Best Practices](https://redis.io/docs/manual/patterns/)

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-06
**ì‘ì„±ì**: SKKU-INSIGHT ê°œë°œíŒ€
