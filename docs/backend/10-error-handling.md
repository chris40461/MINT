# ì—ëŸ¬ ì²˜ë¦¬ (Error Handling)

## ğŸ“Œ ë¬¸ì„œ ëª©ì 

ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ìœ„í•œ ì—ëŸ¬ ì²˜ë¦¬ ì „ëµ, ì¬ì‹œë„ ë¡œì§, ë¡œê¹…, ëª¨ë‹ˆí„°ë§ ë°©ë²•ì„ ì •ì˜í•©ë‹ˆë‹¤.

---

## ğŸ¯ ì—ëŸ¬ ë¶„ë¥˜

### 1. ë°ì´í„° ìˆ˜ì§‘ ì—ëŸ¬

| ì—ëŸ¬ íƒ€ì… | ì›ì¸ | ì²˜ë¦¬ ë°©ë²• |
|---------|------|---------|
| **NetworkError** | API ì„œë²„ ë‹¤ìš´, ë„¤íŠ¸ì›Œí¬ ì¥ì•  | 3íšŒ ì¬ì‹œë„ (2ì´ˆ ê°„ê²©) |
| **TimeoutError** | ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ | íƒ€ì„ì•„ì›ƒ ì¦ê°€ í›„ ì¬ì‹œë„ |
| **RateLimitError** | API í˜¸ì¶œ ì œí•œ ì´ˆê³¼ | ì§€ìˆ˜ ë°±ì˜¤í”„ ëŒ€ê¸° |
| **DataNotFoundError** | ë°ì´í„° ì—†ìŒ (íœ´ì¥ì¼, ì‹ ê·œ ìƒì¥) | ìŠ¤í‚µ, ë¡œê·¸ ê¸°ë¡ |
| **InvalidDataError** | ì˜ëª»ëœ í˜•ì‹ì˜ ë°ì´í„° | ê²€ì¦ ì‹¤íŒ¨ ë¡œê·¸, ì•Œë¦¼ |

### 2. LLM ì—ëŸ¬

| ì—ëŸ¬ íƒ€ì… | ì›ì¸ | ì²˜ë¦¬ ë°©ë²• |
|---------|------|---------|
| **APIKeyError** | ì˜ëª»ëœ API í‚¤ | ì¦‰ì‹œ ì¤‘ë‹¨, ì•Œë¦¼ |
| **QuotaExceededError** | API í• ë‹¹ëŸ‰ ì´ˆê³¼ | ëŒ€ê¸° í›„ ì¬ì‹œë„ |
| **ModelOverloadError** | ëª¨ë¸ ê³¼ë¶€í•˜ | ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ |
| **InvalidPromptError** | í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì˜¤ë¥˜ | í”„ë¡¬í”„íŠ¸ ìˆ˜ì • í•„ìš” |
| **ResponseParsingError** | ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨ | ì¬ì‹œë„ ë˜ëŠ” ê¸°ë³¸ê°’ ë°˜í™˜ |

### 3. ë°ì´í„°ë² ì´ìŠ¤ ì—ëŸ¬

| ì—ëŸ¬ íƒ€ì… | ì›ì¸ | ì²˜ë¦¬ ë°©ë²• |
|---------|------|---------|
| **ConnectionError** | DB ì—°ê²° ì‹¤íŒ¨ | ì¬ì—°ê²° ì‹œë„ (5íšŒ) |
| **IntegrityError** | ì¤‘ë³µ í‚¤, ì œì•½ì¡°ê±´ ìœ„ë°˜ | ë¡œê·¸ ê¸°ë¡, ìŠ¤í‚µ |
| **TransactionError** | íŠ¸ëœì­ì…˜ ì‹¤íŒ¨ | ë¡¤ë°± í›„ ì¬ì‹œë„ |
| **DiskFullError** | ë””ìŠ¤í¬ ìš©ëŸ‰ ë¶€ì¡± | ì•Œë¦¼, ì •ë¦¬ ì‘ì—… |

### 4. Redis ìºì‹œ ì—ëŸ¬

| ì—ëŸ¬ íƒ€ì… | ì›ì¸ | ì²˜ë¦¬ ë°©ë²• |
|---------|------|---------|
| **ConnectionError** | Redis ì„œë²„ ë‹¤ìš´ | Fallback to DB |
| **MemoryError** | ë©”ëª¨ë¦¬ ë¶€ì¡± | LRU ì •ì±…ìœ¼ë¡œ ìë™ ì‚­ì œ |
| **KeyNotFoundError** | ìºì‹œ ë¯¸ìŠ¤ | DB ì¡°íšŒ í›„ ìºì‹± |

---

## ğŸ”„ ì¬ì‹œë„ ë¡œì§

### 1. ê¸°ë³¸ ì¬ì‹œë„ ë°ì½”ë ˆì´í„°

```python
# backend/app/utils/retry.py

from functools import wraps
import time
import asyncio
from typing import Callable, Type
import logging

logger = logging.getLogger(__name__)

def retry(
    max_attempts: int = 3,
    delay: float = 2.0,
    backoff: float = 1.0,
    exceptions: tuple = (Exception,)
):
    """
    ì¬ì‹œë„ ë°ì½”ë ˆì´í„°

    Args:
        max_attempts: ìµœëŒ€ ì‹œë„ íšŸìˆ˜
        delay: ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        backoff: ì§€ìˆ˜ ë°±ì˜¤í”„ ë°°ìˆ˜ (1.0 = ê³ ì •, 2.0 = ì§€ìˆ˜)
        exceptions: ì¬ì‹œë„í•  ì˜ˆì™¸ íƒ€ì… íŠœí”Œ

    Example:
        @retry(max_attempts=3, delay=2, backoff=2.0)
        def fetch_data():
            ...
    """
    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            attempt = 0
            current_delay = delay

            while attempt < max_attempts:
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    attempt += 1

                    if attempt >= max_attempts:
                        logger.error(
                            f"{func.__name__} failed after {max_attempts} attempts: {e}",
                            exc_info=True
                        )
                        raise

                    logger.warning(
                        f"{func.__name__} failed (attempt {attempt}/{max_attempts}): {e}. "
                        f"Retrying in {current_delay}s..."
                    )

                    time.sleep(current_delay)
                    current_delay *= backoff

        return wrapper
    return decorator
```

### 2. ë¹„ë™ê¸° ì¬ì‹œë„

```python
def async_retry(
    max_attempts: int = 3,
    delay: float = 2.0,
    backoff: float = 1.0,
    exceptions: tuple = (Exception,)
):
    """
    ë¹„ë™ê¸° í•¨ìˆ˜ìš© ì¬ì‹œë„ ë°ì½”ë ˆì´í„°
    """
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            attempt = 0
            current_delay = delay

            while attempt < max_attempts:
                try:
                    return await func(*args, **kwargs)
                except exceptions as e:
                    attempt += 1

                    if attempt >= max_attempts:
                        logger.error(
                            f"{func.__name__} failed after {max_attempts} attempts: {e}",
                            exc_info=True
                        )
                        raise

                    logger.warning(
                        f"{func.__name__} failed (attempt {attempt}/{max_attempts}): {e}. "
                        f"Retrying in {current_delay}s..."
                    )

                    await asyncio.sleep(current_delay)
                    current_delay *= backoff

        return wrapper
    return decorator
```

### 3. tenacity ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©

```python
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((ConnectionError, TimeoutError))
)
async def fetch_market_data(date: str):
    """
    ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ (ì¬ì‹œë„ í¬í•¨)
    """
    # pykrx í˜¸ì¶œ
    pass
```

---

## ğŸ“ ë¡œê¹… ì „ëµ

### 1. ë¡œê¹… ì„¤ì •

```python
# backend/app/core/logging_config.py

import logging
import logging.handlers
from pathlib import Path

def setup_logging(
    log_level: str = "INFO",
    log_file: str = "./data/logs/app.log"
):
    """
    ë¡œê¹… ì„¤ì •

    ë¡œê·¸ ë ˆë²¨:
    - DEBUG: ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´
    - INFO: ì¼ë°˜ ì •ë³´ (ë°°ì¹˜ ì‹¤í–‰, API í˜¸ì¶œ)
    - WARNING: ê²½ê³  (ì¬ì‹œë„, ìºì‹œ ë¯¸ìŠ¤)
    - ERROR: ì—ëŸ¬ (API ì‹¤íŒ¨, DB ì˜¤ë¥˜)
    - CRITICAL: ì¹˜ëª…ì  ì˜¤ë¥˜ (ì„œë¹„ìŠ¤ ì¤‘ë‹¨)
    """
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_path = Path(log_file)
    log_path.parent.mkdir(parents=True, exist_ok=True)

    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    logger = logging.getLogger()
    logger.setLevel(getattr(logging, log_level.upper()))

    # í¬ë§·í„°
    formatter = logging.Formatter(
        fmt='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)

    # íŒŒì¼ í•¸ë“¤ëŸ¬ (ìë™ ë¡œí…Œì´ì…˜)
    file_handler = logging.handlers.RotatingFileHandler(
        filename=log_file,
        maxBytes=10 * 1024 * 1024,  # 10MB
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)

    # í•¸ë“¤ëŸ¬ ì¶”ê°€
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

    return logger
```

### 2. ë¡œê¹… ì˜ˆì‹œ

```python
import logging

logger = logging.getLogger(__name__)

# ì •ë³´ ë¡œê·¸
logger.info("Morning triggers started")

# ê²½ê³  ë¡œê·¸
logger.warning(f"Cache miss for ticker {ticker}")

# ì—ëŸ¬ ë¡œê·¸ (ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í¬í•¨)
try:
    result = fetch_data()
except Exception as e:
    logger.error(f"Data fetch failed: {e}", exc_info=True)

# ë””ë²„ê·¸ ë¡œê·¸
logger.debug(f"Composite score calculated: {score}")
```

---

## ğŸš¨ ì˜ˆì™¸ ì²˜ë¦¬ íŒ¨í„´

### 1. ì»¤ìŠ¤í…€ ì˜ˆì™¸ ì •ì˜

```python
# backend/app/core/exceptions.py

class MintException(Exception):
    """
    í”„ë¡œì íŠ¸ ê¸°ë³¸ ì˜ˆì™¸
    """
    pass

class DataCollectionError(MintException):
    """
    ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨
    """
    pass

class LLMServiceError(MintException):
    """
    LLM ì„œë¹„ìŠ¤ ì˜¤ë¥˜
    """
    pass

class CacheError(MintException):
    """
    ìºì‹œ ì˜¤ë¥˜
    """
    pass

class AnalysisError(MintException):
    """
    ë¶„ì„ ì‹¤íŒ¨
    """
    pass
```

### 2. FastAPI ì˜ˆì™¸ í•¸ë“¤ëŸ¬

```python
# backend/app/main.py

from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from app.core.exceptions import MintException
import logging

logger = logging.getLogger(__name__)

app = FastAPI()

@app.exception_handler(MintException)
async def skku_exception_handler(request: Request, exc: MintException):
    """
    í”„ë¡œì íŠ¸ ì»¤ìŠ¤í…€ ì˜ˆì™¸ í•¸ë“¤ëŸ¬
    """
    logger.error(f"Custom exception: {exc}", exc_info=True)

    return JSONResponse(
        status_code=500,
        content={
            "error": exc.__class__.__name__,
            "message": str(exc),
            "path": str(request.url)
        }
    )

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """
    ì „ì—­ ì˜ˆì™¸ í•¸ë“¤ëŸ¬
    """
    logger.critical(f"Unhandled exception: {exc}", exc_info=True)

    return JSONResponse(
        status_code=500,
        content={
            "error": "InternalServerError",
            "message": "An unexpected error occurred",
            "path": str(request.url)
        }
    )
```

### 3. ì„œë¹„ìŠ¤ ë ˆì´ì–´ ì—ëŸ¬ ì²˜ë¦¬

```python
# backend/app/services/trigger_service.py

async def run_morning_triggers(date: datetime) -> List[Dict]:
    """
    ì˜¤ì „ íŠ¸ë¦¬ê±° ì‹¤í–‰ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)
    """
    try:
        # ë°ì´í„° ìˆ˜ì§‘
        try:
            current_data = await data_service.get_market_snapshot(date)
        except ConnectionError as e:
            logger.error(f"Failed to fetch market data: {e}")
            raise DataCollectionError(f"Market data unavailable for {date}")

        # íŠ¸ë¦¬ê±° ì‹¤í–‰
        results = []

        for trigger_func in [morning_volume_surge, morning_gap_up, morning_fund_inflow]:
            try:
                trigger_results = await trigger_func(current_data)
                results.extend(trigger_results)
            except Exception as e:
                logger.error(f"Trigger {trigger_func.__name__} failed: {e}", exc_info=True)
                # ì¼ë¶€ íŠ¸ë¦¬ê±° ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                continue

        if not results:
            logger.warning("No triggers produced results")

        return results

    except DataCollectionError:
        # ìƒìœ„ë¡œ ì „íŒŒ
        raise
    except Exception as e:
        logger.critical(f"Unexpected error in run_morning_triggers: {e}", exc_info=True)
        raise MintException("Morning triggers failed")
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### 1. í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸

```python
# backend/app/api/v1/health.py

from fastapi import APIRouter
from app.services.data_service import DataService
from app.core.cache import redis_client
from app.db.session import get_db

router = APIRouter()

@router.get("/health")
async def health_check():
    """
    ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬

    Returns:
        {
            "status": "healthy" | "degraded" | "unhealthy",
            "services": {
                "database": "ok" | "error",
                "redis": "ok" | "error",
                "pykrx": "ok" | "error"
            }
        }
    """
    services = {}

    # ë°ì´í„°ë² ì´ìŠ¤ ì²´í¬
    try:
        db = next(get_db())
        db.execute("SELECT 1")
        services['database'] = 'ok'
    except Exception as e:
        services['database'] = 'error'
        logger.error(f"Database health check failed: {e}")

    # Redis ì²´í¬
    try:
        redis_client.ping()
        services['redis'] = 'ok'
    except Exception as e:
        services['redis'] = 'error'
        logger.error(f"Redis health check failed: {e}")

    # pykrx ì²´í¬
    try:
        from pykrx import stock
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
        services['pykrx'] = 'ok'
    except Exception as e:
        services['pykrx'] = 'error'
        logger.error(f"pykrx health check failed: {e}")

    # ì „ì²´ ìƒíƒœ íŒë‹¨
    if all(v == 'ok' for v in services.values()):
        status = 'healthy'
    elif any(v == 'ok' for v in services.values()):
        status = 'degraded'
    else:
        status = 'unhealthy'

    return {
        "status": status,
        "services": services
    }
```

### 2. ì—ëŸ¬ ì•Œë¦¼ (ì´ë©”ì¼/Slack)

```python
# backend/app/utils/notifications.py

import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import httpx
import logging

logger = logging.getLogger(__name__)

async def send_error_notification(
    error_type: str,
    error_message: str,
    traceback: str = None
):
    """
    ì—ëŸ¬ ì•Œë¦¼ ë°œì†¡

    Args:
        error_type: ì—ëŸ¬ ìœ í˜• (ì˜ˆ: "DataCollectionError")
        error_message: ì—ëŸ¬ ë©”ì‹œì§€
        traceback: ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ (ì„ íƒ)
    """
    # Slack ì›¹í›… (ì„ íƒ)
    slack_webhook_url = os.getenv("SLACK_WEBHOOK_URL")

    if slack_webhook_url:
        try:
            payload = {
                "text": f"âš ï¸ MINT ì—ëŸ¬ ë°œìƒ",
                "blocks": [
                    {
                        "type": "section",
                        "text": {
                            "type": "mrkdwn",
                            "text": f"*ì—ëŸ¬ íƒ€ì…*: {error_type}\n*ë©”ì‹œì§€*: {error_message}"
                        }
                    }
                ]
            }

            if traceback:
                payload["blocks"].append({
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": f"```{traceback[:500]}```"  # ì²˜ìŒ 500ìë§Œ
                    }
                })

            async with httpx.AsyncClient() as client:
                response = await client.post(slack_webhook_url, json=payload)

                if response.status_code != 200:
                    logger.error(f"Slack notification failed: {response.text}")

        except Exception as e:
            logger.error(f"Failed to send Slack notification: {e}")
```

---

## ğŸ”§ Fallback ì „ëµ

### 1. Redis ìºì‹œ Fallback

```python
async def get_analysis(ticker: str) -> dict:
    """
    ê¸°ì—… ë¶„ì„ ì¡°íšŒ (Redis ì‹¤íŒ¨ ì‹œ DBë¡œ Fallback)
    """
    # 1ì°¨: Redis ìºì‹œ ì‹œë„
    try:
        cached = await redis_client.get(f"analysis:{ticker}")
        if cached:
            return json.loads(cached)
    except Exception as e:
        logger.warning(f"Redis cache failed, falling back to DB: {e}")

    # 2ì°¨: DB ì¡°íšŒ
    try:
        result = await db.query("SELECT * FROM analysis WHERE ticker = ?", (ticker,))
        if result:
            return result
    except Exception as e:
        logger.error(f"DB query failed: {e}")

    # 3ì°¨: LLM ìƒì„±
    return await llm_service.analyze_company(ticker)
```

### 2. ë°ì´í„° ì†ŒìŠ¤ Fallback

```python
async def get_stock_price(ticker: str) -> float:
    """
    ì¢…ëª© ê°€ê²© ì¡°íšŒ (ì—¬ëŸ¬ ì†ŒìŠ¤ ì‹œë„)
    """
    # 1ì°¨: pykrx
    try:
        price = await fetch_price_from_pykrx(ticker)
        if price:
            return price
    except Exception as e:
        logger.warning(f"pykrx failed: {e}")

    # 2ì°¨: MCP ì„œë²„ (kospi_kosdaq)
    try:
        price = await fetch_price_from_mcp(ticker)
        if price:
            return price
    except Exception as e:
        logger.warning(f"MCP server failed: {e}")

    # 3ì°¨: ìºì‹œëœ ê³¼ê±° ë°ì´í„°
    try:
        price = await get_cached_price(ticker)
        if price:
            logger.warning(f"Using cached price for {ticker}")
            return price
    except Exception as e:
        logger.error(f"All price sources failed for {ticker}")

    raise DataCollectionError(f"Unable to fetch price for {ticker}")
```

---

## ğŸ“ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
# backend/app/utils/metrics.py

from prometheus_client import Counter, Histogram, Gauge
import time

# ì¹´ìš´í„°
api_requests_total = Counter(
    'api_requests_total',
    'Total API requests',
    ['method', 'endpoint', 'status']
)

llm_calls_total = Counter(
    'llm_calls_total',
    'Total LLM API calls',
    ['model', 'status']
)

data_collection_errors = Counter(
    'data_collection_errors_total',
    'Total data collection errors',
    ['source']
)

# íˆìŠ¤í† ê·¸ë¨
api_response_time = Histogram(
    'api_response_time_seconds',
    'API response time',
    ['endpoint']
)

llm_generation_time = Histogram(
    'llm_generation_time_seconds',
    'LLM generation time',
    ['prompt_type']
)

# ê²Œì´ì§€
active_triggers = Gauge(
    'active_triggers',
    'Number of active triggers',
    ['session']
)

cache_hit_rate = Gauge(
    'cache_hit_rate',
    'Cache hit rate',
    ['cache_type']
)
```

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-06
**ì‘ì„±ì**: MINT ê°œë°œíŒ€
